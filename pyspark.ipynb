{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpmsiva/pysaprk-jupyter-note-book/blob/main/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "9V8LjsrU4ECV",
        "outputId": "fdb2ee92-3d17-45b1-c93a-5e0c56b52fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [W\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [Waiting for headers] [W\u001b[0m\r                                                                               \rIgn:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\u001b[0m\r                                                                               \rHit:4 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\u001b[0m\r                                                                               \rHit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d5bd3620160>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://58618d789ef7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark Example\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XOSt82tN5S8f",
        "outputId": "00355047-a965-4512-afef-8dcdf24bc77b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdpf2EZd5yhU",
        "outputId": "165dbf79-30ec-4950-ab2e-02396f47b8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|        _c0|      _c1|               _c2|        _c3|           _c4|        _c5|       _c6|          _c7|               _c8|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "|  longitude| latitude|housing_median_age|total_rooms|total_bedrooms| population|households|median_income|median_house_value|\n",
            "|-122.050000|37.370000|         27.000000|3885.000000|    661.000000|1537.000000|606.000000|     6.608500|     344700.000000|\n",
            "|-118.300000|34.260000|         43.000000|1510.000000|    310.000000| 809.000000|277.000000|     3.599000|     176500.000000|\n",
            "|-117.810000|33.780000|         27.000000|3589.000000|    507.000000|1484.000000|495.000000|     5.793400|     270500.000000|\n",
            "|-118.360000|33.820000|         28.000000|  67.000000|     15.000000|  49.000000| 11.000000|     6.135900|     330000.000000|\n",
            "|-119.670000|36.330000|         19.000000|1241.000000|    244.000000| 850.000000|237.000000|     2.937500|      81700.000000|\n",
            "|-119.560000|36.510000|         37.000000|1018.000000|    213.000000| 663.000000|204.000000|     1.663500|      67000.000000|\n",
            "|-121.430000|38.630000|         43.000000|1009.000000|    225.000000| 604.000000|218.000000|     1.664100|      67000.000000|\n",
            "|-120.650000|35.480000|         19.000000|2310.000000|    471.000000|1341.000000|441.000000|     3.225000|     166900.000000|\n",
            "|-122.840000|38.400000|         15.000000|3080.000000|    617.000000|1446.000000|599.000000|     3.669600|     194400.000000|\n",
            "|-118.020000|34.080000|         31.000000|2402.000000|    632.000000|2830.000000|603.000000|     2.333300|     164200.000000|\n",
            "|-118.240000|33.980000|         45.000000| 972.000000|    249.000000|1288.000000|261.000000|     2.205400|     125000.000000|\n",
            "|-119.120000|35.850000|         37.000000| 736.000000|    166.000000| 564.000000|138.000000|     2.416700|      58300.000000|\n",
            "|-121.930000|37.250000|         36.000000|1089.000000|    182.000000| 535.000000|170.000000|     4.690000|     252600.000000|\n",
            "|-117.030000|32.970000|         16.000000|3936.000000|    694.000000|1935.000000|659.000000|     4.562500|     231200.000000|\n",
            "|-117.970000|33.730000|         27.000000|2097.000000|    325.000000|1217.000000|331.000000|     5.712100|     222500.000000|\n",
            "|-117.990000|33.810000|         42.000000| 161.000000|     40.000000| 157.000000| 50.000000|     2.200000|     153100.000000|\n",
            "|-120.810000|37.530000|         15.000000| 570.000000|    123.000000| 189.000000|107.000000|     1.875000|     181300.000000|\n",
            "|-121.200000|38.690000|         26.000000|3077.000000|    607.000000|1603.000000|595.000000|     2.717400|     137500.000000|\n",
            "|-118.880000|34.210000|         26.000000|1590.000000|    196.000000| 654.000000|199.000000|     6.585100|     300000.000000|\n",
            "+-----------+---------+------------------+-----------+--------------+-----------+----------+-------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df=spark.read.csv(\"sample_data/california_housing_test.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2mpSxCF6txl",
        "outputId": "280f2cd5-3294-40de-a071-0a35d6ca3eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3001\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(df.count())\n",
        "len(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnM-ckpl8cTZ"
      },
      "outputs": [],
      "source": [
        "def spark_shape(self):\n",
        "  return(self.count(),len(self.columns))\n",
        "pyspark.sql.dataframe.DataFrame.shape = spark_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWlMOoDp80-K",
        "outputId": "d9f92714-5b1a-4081-a302-0edc329aa56d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3001, 9)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWs_44--9Iz8",
        "outputId": "3d7428e1-2224-4fe5-c09a-34a05eae8693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            "\n",
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|    James|   Smith|    USA|   CA|\n",
            "|  Michael|    Rose|    USA|   NY|\n",
            "|   Robert|Williams|    USA|   CA|\n",
            "|    Maria|   Jones|    USA|   FL|\n",
            "+---------+--------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
        "broadcastStates=spark.sparkContext.broadcast(states)\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "df= spark.createDataFrame(data=data,schema=columns)\n",
        "df.printSchema()\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLuntegI_i_V",
        "outputId": "1d8049d8-5790-4a6f-8962-822a0ae8a222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+-------+-----+----------+\n",
            "|firstname|lastname|country|state|        _5|\n",
            "+---------+--------+-------+-----+----------+\n",
            "|    James|   Smith|    USA|   CA|California|\n",
            "|  Michael|    Rose|    USA|   NY|  New York|\n",
            "|   Robert|Williams|    USA|   CA|California|\n",
            "|    Maria|   Jones|    USA|   FL|   Florida|\n",
            "+---------+--------+-------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def state_converter(code):\n",
        "  return broadcastStates.value[code]\n",
        "\n",
        "result= df.rdd.map(lambda x: (x[0],x[1],x[2],x[3],state_converter(x[3]))).toDF(columns)\n",
        "result.show()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFSX46V7FXaI"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataDictionary = [\n",
        "        ('James',{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',{'hair':'brown','eye':None}),\n",
        "        ('Robert',{'hair':'red','eye':'black'}),\n",
        "        ('Washington',{'hair':'red','eye':'grey'}),\n",
        "        ('Jefferson',{'hair':'red','eye':''})\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apzLtNLgFe-x",
        "outputId": "60264b1c-c804-495e-928c-8f9b31d02c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+-----------------------------+\n",
            "|name      |properties                   |\n",
            "+----------+-----------------------------+\n",
            "|James     |{eye -> brown, hair -> black}|\n",
            "|Michael   |{eye -> NULL, hair -> brown} |\n",
            "|Robert    |{eye -> black, hair -> red}  |\n",
            "|Washington|{eye -> grey, hair -> red}   |\n",
            "|Jefferson |{eye -> , hair -> red}       |\n",
            "+----------+-----------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= spark.createDataFrame(data=dataDictionary, schema=[\"name\",\"properties\"])\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6zS7clxGUvd",
        "outputId": "d7080d9a-0266-4f78-c63d-b62293ee85cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----------------------------+\n",
            "|name      |properties                   |\n",
            "+----------+-----------------------------+\n",
            "|James     |{eye -> brown, hair -> black}|\n",
            "|Michael   |{eye -> NULL, hair -> brown} |\n",
            "|Robert    |{eye -> black, hair -> red}  |\n",
            "|Washington|{eye -> grey, hair -> red}   |\n",
            "|Jefferson |{eye -> , hair -> red}       |\n",
            "+----------+-----------------------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType,StructField,StringType,MapType\n",
        "schema= StructType([\n",
        "    StructField('name',StringType(),True),\n",
        "    StructField('properties',MapType(StringType(),StringType(),True))])\n",
        "df2=spark.createDataFrame(data=dataDictionary,schema=schema)\n",
        "df2.show(truncate=False)\n",
        "\n",
        "df2.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hALQQSuK6mj",
        "outputId": "0c968c34-4fa1-460b-eb21-e7305638b717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+\n",
            "|      name| hair|  eye|\n",
            "+----------+-----+-----+\n",
            "|     James|black|brown|\n",
            "|   Michael|brown| NULL|\n",
            "|    Robert|  red|black|\n",
            "|Washington|  red| grey|\n",
            "| Jefferson|  red|     |\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.rdd.map(lambda x: (x.name,x.properties[\"hair\"],x.properties[\"eye\"])).toDF([\"name\",\"hair\",\"eye\"]).show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHGZMI3mNuu_",
        "outputId": "5eaceaac-ba4f-43eb-e6af-71d3c832d1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        }
      ],
      "source": [
        "accum=spark.sparkContext.accumulator(0)\n",
        "rdd= spark.sparkContext.parallelize([1,2,3,4,5])\n",
        "rdd.foreach(lambda x:accum.add(x))\n",
        "print(accum.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nds-wwWQOugu",
        "outputId": "e00eeb40-a746-416f-f82e-4de6fe3402c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35\n"
          ]
        }
      ],
      "source": [
        "def countfun(x):\n",
        "  global accum\n",
        "  accum+=x\n",
        "rdd.foreach(countfun)\n",
        "print(accum.value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfP57HtNS_pu",
        "outputId": "a78992cc-407b-4fa6-8ad0-cc2dd5449428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|James    |Smith   |USA    |CA   |\n",
            "|Michael  |Rose    |USA    |NY   |\n",
            "|Robert   |Williams|USA    |CA   |\n",
            "|Maria    |Jones   |USA    |FL   |\n",
            "+---------+--------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
        "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
        "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
        "  ]\n",
        "\n",
        "# Column names\n",
        "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWbrm9HETSDg",
        "outputId": "4d5b62c9-9427-4d3f-c318-9260adee88fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|    James|   Smith|    USA|   CA|\n",
            "|  Michael|    Rose|    USA|   NY|\n",
            "|   Robert|Williams|    USA|   CA|\n",
            "|    Maria|   Jones|    USA|   FL|\n",
            "+---------+--------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(*columns).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1bSCKclT8Gd",
        "outputId": "1d1b31c4-2bdf-4c65-ce7b-15b886de406e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+-----+------+\n",
            "|name                  |state|gender|\n",
            "+----------------------+-----+------+\n",
            "|{James, NULL, Smith}  |OH   |M     |\n",
            "|{Anna, Rose, }        |NY   |F     |\n",
            "|{Julia, , Williams}   |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |NY   |M     |\n",
            "|{Mike, Mary, Williams}|OH   |M     |\n",
            "+----------------------+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True)\n",
        "     ])\n",
        "\n",
        "\n",
        "data = [((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
        "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
        "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
        "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
        "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
        "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
        "        ]\n",
        "\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o010rksjUU3G",
        "outputId": "643481d2-2cb9-4eea-9606-11d9812914d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+-----+------+\n",
            "|firstname|middlename|lastname|state|gender|\n",
            "+---------+----------+--------+-----+------+\n",
            "|James    |NULL      |Smith   |OH   |M     |\n",
            "|Anna     |Rose      |        |NY   |F     |\n",
            "|Julia    |          |Williams|OH   |F     |\n",
            "|Maria    |Anne      |Jones   |NY   |M     |\n",
            "|Jen      |Mary      |Brown   |NY   |M     |\n",
            "|Mike     |Mary      |Williams|OH   |M     |\n",
            "+---------+----------+--------+-----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.select('name.*',\"state\",\"gender\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OtmhIo_JCOvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDHIFs0yrNnh"
      },
      "outputs": [],
      "source": [
        "data = [('James','','Smith','1991-04-01','M',3000),\n",
        "  ('Michael','Rose','','2000-05-19','M',4000),\n",
        "  ('Robert','','Williams','1978-09-05','M',4000),\n",
        "  ('Maria','Anne','Jones','1967-12-01','F',4000),\n",
        "  ('Jen','Mary','Brown','1980-02-17','F',-1)\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wct4zIOgrS01"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession.builder.appName(\"siva\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rkc74ZeGtk3a",
        "outputId": "8fd95623-b703-403f-8e38-203bd22a9e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|dob       |gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|James    |          |Smith   |1991-04-01|M     |3000  |\n",
            "|Michael  |Rose      |        |2000-05-19|M     |4000  |\n",
            "|Robert   |          |Williams|1978-09-05|M     |4000  |\n",
            "|Maria    |Anne      |Jones   |1967-12-01|F     |4000  |\n",
            "|Jen      |Mary      |Brown   |1980-02-17|F     |-1    |\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= spark.createDataFrame(data=data,schema=columns)\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOkbxyptuMLM",
        "outputId": "18a88e8c-ec69-4189-fb1f-ebcd909b11ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- dob: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8iIyfo_urLg",
        "outputId": "ad377387-1f7c-4907-96d6-2d44ea502ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "df.withColumn(\"salary\",col(\"salary\").cast(\"Integer\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6j21ISDwEIp",
        "outputId": "e21f0f5c-c65b-4f8c-8947-cb9359a90bdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "|    James|          |   Smith|1991-04-01|     M|300000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|400000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|400000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|400000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|  -100|\n",
            "+---------+----------+--------+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"salary\",col(\"salary\")*100).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqLJ24jzwWPR",
        "outputId": "c7c7baa5-b05c-4b56-e6a1-26bee0dfd950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+----------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|new_salary|\n",
            "+---------+----------+--------+----------+------+------+----------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|     36000|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|     48000|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|     48000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|     48000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|       -12|\n",
            "+---------+----------+--------+----------+------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"new_salary\",col(\"salary\")*12).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZY9NPRew6ZE",
        "outputId": "9e6c0229-9195-4a44-efe9-3863dbb4a13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+------+------+-------+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|country|\n",
            "+---------+----------+--------+----------+------+------+-------+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|  india|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|  india|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|  india|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|  india|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|  india|\n",
            "+---------+----------+--------+----------+------+------+-------+\n",
            "\n",
            "+---------+----------+--------+----------+------+------+-------+-----+\n",
            "|firstname|middlename|lastname|       dob|gender|salary|country|state|\n",
            "+---------+----------+--------+----------+------+------+-------+-----+\n",
            "|    James|          |   Smith|1991-04-01|     M|  3000|  india|   TN|\n",
            "|  Michael|      Rose|        |2000-05-19|     M|  4000|  india|   TN|\n",
            "|   Robert|          |Williams|1978-09-05|     M|  4000|  india|   TN|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|     F|  4000|  india|   TN|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|     F|    -1|  india|   TN|\n",
            "+---------+----------+--------+----------+------+------+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df.withColumn(\"country\",lit(\"india\")).show()\n",
        "df.withColumn(\"country\",lit(\"india\"))\\\n",
        "  .withColumn(\"state\",lit(\"TN\"))\\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XznYXrYDyLu7",
        "outputId": "10b40e4b-78e8-4da2-ab25-36b43e83806e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+--------+----------+---+------+\n",
            "|firstname|middlename|lastname|       dob|sex|salary|\n",
            "+---------+----------+--------+----------+---+------+\n",
            "|    James|          |   Smith|1991-04-01|  M|  3000|\n",
            "|  Michael|      Rose|        |2000-05-19|  M|  4000|\n",
            "|   Robert|          |Williams|1978-09-05|  M|  4000|\n",
            "|    Maria|      Anne|   Jones|1967-12-01|  F|  4000|\n",
            "|      Jen|      Mary|   Brown|1980-02-17|  F|    -1|\n",
            "+---------+----------+--------+----------+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumnRenamed(\"gender\",\"sex\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZxms8wKzOhw"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
        "\n",
        "schema= StructType([\n",
        "    StructField('fname',StructType([\n",
        "        StructField('fname',StringType(),True),\n",
        "        StructField('mname',StringType(),True),\n",
        "        StructField('lname',StringType(),True),\n",
        "        ])),\n",
        "    StructField('dob',StringType(),True),\n",
        "    StructField('gender',StringType(),True),\n",
        "    StructField('salary',StringType(),True),\n",
        "    ])\n",
        "\n",
        "dataDF = [(('James','','Smith'),'1991-04-01','M',3000),\n",
        "  (('Michael','Rose',''),'2000-05-19','M',4000),\n",
        "  (('Robert','','Williams'),'1978-09-05','M',4000),\n",
        "  (('Maria','Anne','Jones'),'1967-12-01','F',4000),\n",
        "  (('Jen','Mary','Brown'),'1980-02-17','F',-1)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNioRRFL5GAg",
        "outputId": "6eeaeb65-a1de-4305-9673-c9511fb30d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----------+------+------+\n",
            "|               fname|       dob|gender|salary|\n",
            "+--------------------+----------+------+------+\n",
            "|    {James, , Smith}|1991-04-01|     M|  3000|\n",
            "|   {Michael, Rose, }|2000-05-19|     M|  4000|\n",
            "|{Robert, , Williams}|1978-09-05|     M|  4000|\n",
            "|{Maria, Anne, Jones}|1967-12-01|     F|  4000|\n",
            "|  {Jen, Mary, Brown}|1980-02-17|     F|    -1|\n",
            "+--------------------+----------+------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df= spark.createDataFrame(data=dataDF,schema=schema).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "taaT1aZb44Lp",
        "outputId": "9521b7fc-dafc-4a7d-a051-4a6731b7ae8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "path=os.getcwd()\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVz56U2P_Y4F"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "spark = SparkSession.builder.appName(\"siva\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600), \\\n",
        "    (\"Robert\", \"Sales\", 4100), \\\n",
        "    (\"Maria\", \"Finance\", 3000), \\\n",
        "    (\"James\", \"Sales\", 3000), \\\n",
        "    (\"Scott\", \"Finance\", 3300), \\\n",
        "    (\"Jen\", \"Finance\", 3900), \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000), \\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  ]\n",
        "\n",
        "# Create DataFrame\n",
        "column= [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data=data, schema=column)\n",
        "df.show()x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEubfDFO_zzw",
        "outputId": "5f0a5053-e437-4b01-c740-2d19053c6fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|        James|     Sales|  3000|\n",
            "|      Michael|     Sales|  4600|\n",
            "|       Robert|     Sales|  4100|\n",
            "|        Maria|   Finance|  3000|\n",
            "|        James|     Sales|  3000|\n",
            "|        Scott|   Finance|  3300|\n",
            "|          Jen|   Finance|  3900|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|        Kumar| Marketing|  2000|\n",
            "|         Saif|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disdf=df.distinct()\n",
        "print(disdf.count())\n",
        "\n",
        "disdf.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiLsksGOBrC-",
        "outputId": "4aca0b24-ddad-4f48-88ba-a5aafc89c715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|      Michael|     Sales|  4600|\n",
            "|        James|     Sales|  3000|\n",
            "|       Robert|     Sales|  4100|\n",
            "|        Maria|   Finance|  3000|\n",
            "|          Jen|   Finance|  3900|\n",
            "|        Scott|   Finance|  3300|\n",
            "|        Kumar| Marketing|  2000|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|         Saif|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dupdf=df.dropDuplicates()\n",
        "print(dupdf.count())\n",
        "\n",
        "dupdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsMfPvWgEkCl",
        "outputId": "cea55179-a7fe-4b1f-f8f8-b269405ba6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|      Michael|     Sales|  4600|\n",
            "|        James|     Sales|  3000|\n",
            "|       Robert|     Sales|  4100|\n",
            "|        Maria|   Finance|  3000|\n",
            "|          Jen|   Finance|  3900|\n",
            "|        Scott|   Finance|  3300|\n",
            "|        Kumar| Marketing|  2000|\n",
            "|         Jeff| Marketing|  3000|\n",
            "|         Saif|     Sales|  4100|\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "# Create DataFrame df1 with columns name, and id\n",
        "data = [(\"James\",34), (\"Michael\",56), \\\n",
        "        (\"Robert\",30), (\"Maria\",24) ]\n",
        "\n",
        "df1 = spark.createDataFrame(data = data, schema=[\"name\",\"id\"])\n",
        "df1.printSchema()\n",
        "\n",
        "# Create DataFrame df2 with columns name and id\n",
        "data2=[(34,\"James\"),(45,\"Maria\"), \\\n",
        "       (45,\"Jen\"),(34,\"Jeff\")]\n",
        "\n",
        "df2 = spark.createDataFrame(data = data2, schema = [\"id\",\"name\"])\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOKLcrD6XUBI",
        "outputId": "459e118f-5b3d-4f51-f1a8-6cbe7905ef98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df1.unionByName(df2)\n",
        "df3.show()\n",
        "df3.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RCHWMnsXZUW",
        "outputId": "92ef270b-6fb3-4b17-d1ca-2b4d2c9157d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name| id|\n",
            "+-------+---+\n",
            "|  James| 34|\n",
            "|Michael| 56|\n",
            "| Robert| 30|\n",
            "|  Maria| 24|\n",
            "|  James| 34|\n",
            "|  Maria| 45|\n",
            "|    Jen| 45|\n",
            "|   Jeff| 34|\n",
            "+-------+---+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = spark.createDataFrame([[5, 2, 6]], [\"col0\", \"col1\", \"col2\"])\n",
        "df2 = spark.createDataFrame([[6, 7, 3]], [\"col1\", \"col2\", \"col3\"])\n",
        "\n",
        "\n",
        "df1.show()\n",
        "df2.show()\n",
        "# Using allowMissingColumns\n",
        "df3 = df1.unionByName(df2, allowMissingColumns=True)\n",
        "df3.printSchema\n",
        "df3.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p9yvNgWYR5Z",
        "outputId": "078a9c78-a776-4122-e71c-1a4c093af9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col0|col1|col2|\n",
            "+----+----+----+\n",
            "|   5|   2|   6|\n",
            "+----+----+----+\n",
            "\n",
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|   6|   7|   3|\n",
            "+----+----+----+\n",
            "\n",
            "+----+----+----+----+\n",
            "|col0|col1|col2|col3|\n",
            "+----+----+----+----+\n",
            "|   5|   2|   6|NULL|\n",
            "|NULL|   6|   7|   3|\n",
            "+----+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "            .appName('SparkByExamples.com') \\\n",
        "            .getOrCreate()\n",
        "\n",
        "# Prepare Data\n",
        "simpleData = ((\"Java\",4000,5), \\\n",
        "    (\"Python\", 4600,10),  \\\n",
        "    (\"Scala\", 4100,15),   \\\n",
        "    (\"Scala\", 4500,15),   \\\n",
        "    (\"PHP\", 3000,20),  \\\n",
        "  )\n",
        "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKr2fPYWaxRH",
        "outputId": "196485a9-953d-45b2-ebf8-d84c90394232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CourseName: string (nullable = true)\n",
            " |-- fee: long (nullable = true)\n",
            " |-- discount: long (nullable = true)\n",
            "\n",
            "+----------+----+--------+\n",
            "|CourseName|fee |discount|\n",
            "+----------+----+--------+\n",
            "|Java      |4000|5       |\n",
            "|Python    |4600|10      |\n",
            "|Scala     |4100|15      |\n",
            "|Scala     |4500|15      |\n",
            "|PHP       |3000|20      |\n",
            "+----------+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "            .appName('SparkByExamples.com') \\\n",
        "            .getOrCreate()\n",
        "\n",
        "# Prepare Data\n",
        "simpleData = ((\"Java\",4000,5), \\\n",
        "    (\"Python\", 4600,10),  \\\n",
        "    (\"Scala\", 4100,15),   \\\n",
        "    (\"Scala\", 4500,15),   \\\n",
        "    (\"PHP\", 3000,20),  \\\n",
        "  )\n",
        "columns= [\"CourseName\", \"fee\", \"discount\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kd2hJxJcFhJ",
        "outputId": "61bc5b81-0673-46ee-d510-ef57f6ee3f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CourseName: string (nullable = true)\n",
            " |-- fee: long (nullable = true)\n",
            " |-- discount: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import upper\n",
        "def to_upper_str_columns(df):\n",
        "    return df.withColumn(\"CourseName\",upper(df.CourseName))\n",
        "\n",
        "# Custom transformation 2\n",
        "def reduce_price(df,reduceBy):\n",
        "    return df.withColumn(\"new_fee\",df.fee - reduceBy)\n",
        "\n",
        "# Custom transformation 3\n",
        "def apply_discount(df):\n",
        "    return df.withColumn(\"discounted_fee\",  \\\n",
        "             df.new_fee - (df.new_fee * df.discount) / 100)\n",
        "\n",
        "# transform() usage\n",
        "df2 = df.transform(to_upper_str_columns) \\\n",
        "        .transform(reduce_price,1000) \\\n",
        "        .transform(apply_discount)\n",
        "\n",
        "df2.show()\n",
        "\n",
        "# Create DataFrame with Array\n",
        "data = [\n",
        " (\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],[\"Spark\",\"Java\"]),\n",
        " (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],[\"Spark\",\"Java\"]),\n",
        " (\"Robert,,Williams\",[\"CSharp\",\"VB\"],[\"Spark\",\"Python\"])\n",
        "]\n",
        "df = spark.createDataFrame(data=data,schema=[\"Name\",\"Languages1\",\"Languages2\"])\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jz0bHyLDddRJ",
        "outputId": "844a777c-f692-47bd-fe01-43ffd3ac7a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+--------+-------+--------------+\n",
            "|CourseName| fee|discount|new_fee|discounted_fee|\n",
            "+----------+----+--------+-------+--------------+\n",
            "|      JAVA|4000|       5|   3000|        2850.0|\n",
            "|    PYTHON|4600|      10|   3600|        3240.0|\n",
            "|     SCALA|4100|      15|   3100|        2635.0|\n",
            "|     SCALA|4500|      15|   3500|        2975.0|\n",
            "|       PHP|3000|      20|   2000|        1600.0|\n",
            "+----------+----+--------+-------+--------------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Languages1: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- Languages2: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------+------------------+---------------+\n",
            "|            Name|        Languages1|     Languages2|\n",
            "+----------------+------------------+---------------+\n",
            "|    James,,Smith|[Java, Scala, C++]|  [Spark, Java]|\n",
            "|   Michael,Rose,|[Spark, Java, C++]|  [Spark, Java]|\n",
            "|Robert,,Williams|      [CSharp, VB]|[Spark, Python]|\n",
            "+----------------+------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-TQSCPnfMNl",
        "outputId": "4cc65084-7fc8-41d0-f2fd-160fc5374039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |john jones  |\n",
            "|2    |tracey smith|\n",
            "|3    |amy sanders |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function using withColumn\n",
        "from pyspark.sql.functions import upper\n",
        "df.withColumn(\"Upper_Name\", upper(df.Name)) \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vblVIcT_fm2I",
        "outputId": "6c8c2815-4599-4c30-a212-d464b4bfd654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+------------+\n",
            "|Seqno|        Name|  Upper_Name|\n",
            "+-----+------------+------------+\n",
            "|    1|  john jones|  JOHN JONES|\n",
            "|    2|tracey smith|TRACEY SMITH|\n",
            "|    3| amy sanders| AMY SANDERS|\n",
            "+-----+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function using select\n",
        "df.select(\"Seqno\",\"Name\", upper(df.Name)) \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG_gl3okfq8S",
        "outputId": "8394f3e7-b4b8-4500-d129-3a7197aaea59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+------------+\n",
            "|Seqno|        Name| upper(Name)|\n",
            "+-----+------------+------------+\n",
            "|    1|  john jones|  JOHN JONES|\n",
            "|    2|tracey smith|TRACEY SMITH|\n",
            "|    3| amy sanders| AMY SANDERS|\n",
            "+-----+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"TAB\")\n",
        "spark.sql(\"select Seqno, Name, UPPER(Name) from TAB\") \\\n",
        "     .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmr6dvLEftfc",
        "outputId": "f99ef761-82a6-4d8f-ddd5-50f684d83f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+------------+\n",
            "|Seqno|        Name| upper(Name)|\n",
            "+-----+------------+------------+\n",
            "|    1|  john jones|  JOHN JONES|\n",
            "|    2|tracey smith|TRACEY SMITH|\n",
            "|    3| amy sanders| AMY SANDERS|\n",
            "+-----+------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def upperCase(str):\n",
        "    return str.upper()\n",
        "\n",
        "# Convert function to udf\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "upperCaseUDF = udf(lambda x:upperCase(x),StringType())\n",
        "\n",
        "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "# Custom UDF with select()\n",
        "df.select(col(\"Seqno\"), \\\n",
        "    upperCaseUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "# Custom UDF with sql()\n",
        "spark.udf.register(\"upperCaseUDF\", upperCaseUDF)\n",
        "df.createOrReplaceTempView(\"TAB\")\n",
        "spark.sql(\"select Seqno, Name, upperCaseUDF(Name) from TAB\") \\\n",
        "     .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQpnydvAfwSS",
        "outputId": "aea599d1-fcda-43a2-e39c-b033ab19cc2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+-------------+\n",
            "|Seqno|Name        |Cureated Name|\n",
            "+-----+------------+-------------+\n",
            "|1    |john jones  |JOHN JONES   |\n",
            "|2    |tracey smith|TRACEY SMITH |\n",
            "|3    |amy sanders |AMY SANDERS  |\n",
            "+-----+------------+-------------+\n",
            "\n",
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |JOHN JONES  |\n",
            "|2    |TRACEY SMITH|\n",
            "|3    |AMY SANDERS |\n",
            "+-----+------------+\n",
            "\n",
            "+-----+------------+------------------+\n",
            "|Seqno|        Name|upperCaseUDF(Name)|\n",
            "+-----+------------+------------------+\n",
            "|    1|  john jones|        JOHN JONES|\n",
            "|    2|tracey smith|      TRACEY SMITH|\n",
            "|    3| amy sanders|       AMY SANDERS|\n",
            "+-----+------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports\n",
        "import pyspark.pandas as ps\n",
        "import numpy as np\n",
        "\n",
        "technologies = ({\n",
        "    'Fee' :[20000,25000,30000,22000,np.NaN],\n",
        "    'Discount':[1000,2500,1500,1200,3000]\n",
        "               })\n",
        "# Create a DataFrame\n",
        "psdf = ps.DataFrame(technologies)\n",
        "print(psdf)\n",
        "\n",
        "def add(data):\n",
        "   return data[0] + data[1]\n",
        "\n",
        "addDF = psdf.apply(add,axis=1)\n",
        "print(addDF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVEDIF3YgIrb",
        "outputId": "144da664-15b2-49f3-d74c-f249b9603a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Fee  Discount\n",
            "0  20000.0      1000\n",
            "1  25000.0      2500\n",
            "2  30000.0      1500\n",
            "3  22000.0      1200\n",
            "4      NaN      3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `apply`, it is expensive to infer the data type internally.\n",
            "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
            "<ipython-input-18-34ffc18322ff>:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  return data[0] + data[1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    21000.0\n",
            "1    27500.0\n",
            "2    31500.0\n",
            "3    23200.0\n",
            "4        NaN\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df=spark.range(100)\n",
        "print(df.sample(0.06).collect())\n",
        "dfff=df.sample(0.06).collect()\n",
        "\n",
        "print(dfff)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5_YU9rKm2v1",
        "outputId": "bb8d2b6b-19d4-49e4-bccb-755ab023f500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(id=2), Row(id=3), Row(id=6), Row(id=7), Row(id=40), Row(id=70), Row(id=80), Row(id=88)]\n",
            "[Row(id=65), Row(id=67)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "filePath=\"small_zipcode.csv\"\n",
        "df = spark.read.options(header='true', inferSchema='true') \\\n",
        "          .csv(filePath)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-9dcvKSokok",
        "outputId": "012efb30-8bb3-42f4-b1a0-b23d97da0425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- zipcode: integer (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- population: integer (nullable = true)\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|id |zipcode|type    |city               |state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|1  |704    |STANDARD|NULL               |PR   |30100     |\n",
            "|2  |704    |NULL    |PASEO COSTA DEL SUR|PR   |NULL      |\n",
            "|3  |709    |NULL    |BDA SAN LUIS       |PR   |3700      |\n",
            "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
            "|5  |76177  |STANDARD|NULL               |TX   |NULL      |\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path= os.getcwd()\n",
        "print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HykqzCPSopIz",
        "outputId": "d5c3058d-5d7c-4a4e-b12d-df6a80904b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.fillna(value=0).show()\n",
        "df.fillna(value=0,subset=[\"population\"]).show()\n",
        "df.na.fill(value=0).show()\n",
        "df.na.fill(value=0,subset=[\"population\"]).show()\n",
        "\n",
        "\n",
        "df.fillna(value=\"\").show()\n",
        "df.na.fill(value=\"\").show()\n",
        "\n",
        "df.fillna(\"unknown\",[\"city\"]) \\\n",
        "    .fillna(\"\",[\"type\"]).show()\n",
        "\n",
        "df.fillna({\"city\": \"unknown\", \"type\": \"\"}) \\\n",
        "    .show()\n",
        "\n",
        "df.na.fill(\"unknown\",[\"city\"]) \\\n",
        "    .na.fill(\"\",[\"type\"]).show()\n",
        "\n",
        "df.na.fill({\"city\": \"unknown\", \"type\": \"\"}) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6n_cSLjqLbB",
        "outputId": "7fd062de-5916-4dc4-f5b0-d2b05ecf10ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
            "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
            "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
            "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
            "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
            "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
            "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|               NULL|   PR|     30100|\n",
            "|  2|    704|    NULL|PASEO COSTA DEL SUR|   PR|         0|\n",
            "|  3|    709|    NULL|       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|               NULL|   TX|         0|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|                   |   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|                   |   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|                   |   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|                   |   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|            unknown|   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|            unknown|   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|            unknown|   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "| id|zipcode|    type|               city|state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|  1|    704|STANDARD|            unknown|   PR|     30100|\n",
            "|  2|    704|        |PASEO COSTA DEL SUR|   PR|      NULL|\n",
            "|  3|    709|        |       BDA SAN LUIS|   PR|      3700|\n",
            "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
            "|  5|  76177|STANDARD|            unknown|   TX|      NULL|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "df.na.drop().show(truncate=False)\n",
        "\n",
        "df.na.drop(how=\"any\").show(truncate=False)\n",
        "\n",
        "df.na.drop(subset=[\"population\",\"type\"]) \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "df.dropna().show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0pLcTUdtgRp",
        "outputId": "82d5b121-3f9b-402e-d9a0-5b4f7e34ec99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- zipcode: integer (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- population: integer (nullable = true)\n",
            "\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|id |zipcode|type    |city               |state|population|\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "|1  |704    |STANDARD|NULL               |PR   |30100     |\n",
            "|2  |704    |NULL    |PASEO COSTA DEL SUR|PR   |NULL      |\n",
            "|3  |709    |NULL    |BDA SAN LUIS       |PR   |3700      |\n",
            "|4  |76166  |UNIQUE  |CINGULAR WIRELESS  |TX   |84000     |\n",
            "|5  |76177  |STANDARD|NULL               |TX   |NULL      |\n",
            "+---+-------+--------+-------------------+-----+----------+\n",
            "\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "|id |zipcode|type  |city             |state|population|\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "|4  |76166  |UNIQUE|CINGULAR WIRELESS|TX   |84000     |\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "|id |zipcode|type  |city             |state|population|\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "|4  |76166  |UNIQUE|CINGULAR WIRELESS|TX   |84000     |\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "\n",
            "+---+-------+--------+-----------------+-----+----------+\n",
            "|id |zipcode|type    |city             |state|population|\n",
            "+---+-------+--------+-----------------+-----+----------+\n",
            "|1  |704    |STANDARD|NULL             |PR   |30100     |\n",
            "|4  |76166  |UNIQUE  |CINGULAR WIRELESS|TX   |84000     |\n",
            "+---+-------+--------+-----------------+-----+----------+\n",
            "\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "|id |zipcode|type  |city             |state|population|\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "|4  |76166  |UNIQUE|CINGULAR WIRELESS|TX   |84000     |\n",
            "+---+-------+------+-----------------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import expr\n",
        "#Create spark session\n",
        "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
        "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
        "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
        "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")]\n",
        "\n",
        "columns= [\"Product\",\"Amount\",\"Country\"]\n",
        "df = spark.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Dk1WHsBqN9W",
        "outputId": "3f8a8e56-8406-4982-e3ee-75948a43ab31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Amount: long (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n",
            "+-------+------+-------+\n",
            "|Product|Amount|Country|\n",
            "+-------+------+-------+\n",
            "|Banana |1000  |USA    |\n",
            "|Carrots|1500  |USA    |\n",
            "|Beans  |1600  |USA    |\n",
            "|Orange |2000  |USA    |\n",
            "|Orange |2000  |USA    |\n",
            "|Banana |400   |China  |\n",
            "|Carrots|1200  |China  |\n",
            "|Beans  |1500  |China  |\n",
            "|Orange |4000  |China  |\n",
            "|Banana |2000  |Canada |\n",
            "|Carrots|2000  |Canada |\n",
            "|Beans  |2000  |Mexico |\n",
            "+-------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying pivot()\n",
        "pivotDF = df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
        "pivotDF.printSchema()\n",
        "pivotDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkjw8T8AzK4L",
        "outputId": "fdf14b14-bfa6-47ca-dfec-9ef7a54364be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Product: string (nullable = true)\n",
            " |-- Canada: long (nullable = true)\n",
            " |-- China: long (nullable = true)\n",
            " |-- Mexico: long (nullable = true)\n",
            " |-- USA: long (nullable = true)\n",
            "\n",
            "+-------+------+-----+------+----+\n",
            "|Product|Canada|China|Mexico|USA |\n",
            "+-------+------+-----+------+----+\n",
            "|Orange |NULL  |4000 |NULL  |4000|\n",
            "|Beans  |NULL  |1500 |2000  |1600|\n",
            "|Banana |2000  |400  |NULL  |1000|\n",
            "|Carrots|2000  |1200 |NULL  |1500|\n",
            "+-------+------+-----+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
        "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
        "    .where(\"Total is not null\")\n",
        "unPivotDF.show(truncate=False)\n",
        "unPivotDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq3PzZXa0JCd",
        "outputId": "0172565f-f592-45da-f747-9a9bf31105ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+-----+\n",
            "|Product|Country|Total|\n",
            "+-------+-------+-----+\n",
            "|Orange |China  |4000 |\n",
            "|Beans  |China  |1500 |\n",
            "|Beans  |Mexico |2000 |\n",
            "|Banana |Canada |2000 |\n",
            "|Banana |China  |400  |\n",
            "|Carrots|Canada |2000 |\n",
            "|Carrots|China  |1200 |\n",
            "+-------+-------+-----+\n",
            "\n",
            "+-------+-------+-----+\n",
            "|Product|Country|Total|\n",
            "+-------+-------+-----+\n",
            "| Orange|  China| 4000|\n",
            "|  Beans|  China| 1500|\n",
            "|  Beans| Mexico| 2000|\n",
            "| Banana| Canada| 2000|\n",
            "| Banana|  China|  400|\n",
            "|Carrots| Canada| 2000|\n",
            "|Carrots|  China| 1200|\n",
            "+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame by reading CSV file\n",
        "df=spark.read.option(\"header\",True) \\\n",
        "        .csv(\"simple-zipcodes.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIotb6uS1Nz9",
        "outputId": "5c599c88-d82e-4433-ae68-474f09bd1867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- RecordNumber: string (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Zipcode: string (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.option(\"header\",True) \\\n",
        "        .partitionBy(\"state\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(\"zipcodes-state\")"
      ],
      "metadata": {
        "id": "E-VqqvyX5N7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#partitionBy() multiple columns\n",
        "df.write.option(\"header\",True) \\\n",
        "        .partitionBy(\"state\",\"city\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(\"zipcodes-state\")"
      ],
      "metadata": {
        "id": "EKdeT35Z5uGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.repartition(2).write.option(\"header\",True) \\\n",
        "        .partitionBy(\"state\") \\\n",
        "        .mode(\"overwrite\") \\\n",
        "        .csv(\"zipcodes-state\")"
      ],
      "metadata": {
        "id": "XaVLXtBY6Bai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parq= spark.read.option(\"header\",True).csv(\"zipcodes-state\")\n",
        "parq.createOrReplaceTempView(\"Zipcode\")\n",
        "spark.sql(\"select * from Zipcode where state ='AL'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X361Bly97qr_",
        "outputId": "1cd7b357-be91-495b-8fc4-01ecc6b46c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------+-------------+-------+-----+\n",
            "|RecordNumber|Country|         City|Zipcode|state|\n",
            "+------------+-------+-------------+-------+-----+\n",
            "|       54355|     US|  SPRINGVILLE|  35146|   AL|\n",
            "|       54356|     US|  SPRUCE PINE|  35585|   AL|\n",
            "|       54354|     US|SPRING GARDEN|  36275|   AL|\n",
            "+------------+-------+-------------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
        "schema = StructType([\n",
        "    StructField('name', StringType(), True),\n",
        "    StructField('properties', MapType(StringType(),StringType()),True)\n",
        "])\n",
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "dataDictionary = [\n",
        "        ('James',{'hair':'black','eye':'brown'}),\n",
        "        ('Michael',{'hair':'brown','eye':None}),\n",
        "        ('Robert',{'hair':'red','eye':'black'}),\n",
        "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
        "        ('Jefferson',{'hair':'brown','eye':''})\n",
        "        ]\n",
        "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbL5lFH7CSc8",
        "outputId": "515f54ff-ff91-4225-868a-20277ecf8434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+-----------------------------+\n",
            "|name      |properties                   |\n",
            "+----------+-----------------------------+\n",
            "|James     |{eye -> brown, hair -> black}|\n",
            "|Michael   |{eye -> NULL, hair -> brown} |\n",
            "|Robert    |{eye -> black, hair -> red}  |\n",
            "|Washington|{eye -> grey, hair -> grey}  |\n",
            "|Jefferson |{eye -> , hair -> brown}     |\n",
            "+----------+-----------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df3=df.rdd.map(lambda x: \\\n",
        "#     (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n",
        "#     .toDF([\"name\",\"hair\",\"eye\"])\n",
        "# df3.printSchema()\n",
        "# df3.show()\n",
        "\n",
        "\n",
        "df3= df.rdd.map(lambda x: (x.name,x.properties[\"hair\"],x.properties[\"eye\"]))\\\n",
        "        .toDF([\"name\",\"hair\",\"eye\"])\n",
        "\n",
        "df3.printSchema()\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mLrmlRtETEx",
        "outputId": "7e744cf7-837e-442c-c36f-f591562ada11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- hair: string (nullable = true)\n",
            " |-- eye: string (nullable = true)\n",
            "\n",
            "+----------+-----+-----+\n",
            "|      name| hair|  eye|\n",
            "+----------+-----+-----+\n",
            "|     James|black|brown|\n",
            "|   Michael|brown| NULL|\n",
            "|    Robert|  red|black|\n",
            "|Washington| grey| grey|\n",
            "| Jefferson|brown|     |\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.withColumn(\"hair\",df.properties[\"hair\"]).withColumn(\"eye\",df.properties[\"hair\"]).drop(\"properties\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsfUQoBAGqrM",
        "outputId": "13edec9d-d2d1-49bc-aca1-8bb0609fb44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+\n",
            "|      name| hair|  eye|\n",
            "+----------+-----+-----+\n",
            "|     James|black|black|\n",
            "|   Michael|brown|brown|\n",
            "|    Robert|  red|  red|\n",
            "|Washington| grey| grey|\n",
            "| Jefferson|brown|brown|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df45=df.withColumn(\"hair\",df.properties.getItem(\"hair\")).withColumn(\"eye\",df.properties.getItem(\"hair\")).drop(\"properties\")\n",
        "df45.printSchema()\n",
        "df45.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PIP7OQ1ILRU",
        "outputId": "ee1921c8-35f0-46a5-b3aa-d5864828555f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- hair: string (nullable = true)\n",
            " |-- eye: string (nullable = true)\n",
            "\n",
            "+----------+-----+-----+\n",
            "|      name| hair|  eye|\n",
            "+----------+-----+-----+\n",
            "|     James|black|black|\n",
            "|   Michael|brown|brown|\n",
            "|    Robert|  red|  red|\n",
            "|Washington| grey| grey|\n",
            "| Jefferson|brown|brown|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df.select(df.name,explode(df.properties)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ4CNYuNJVP2",
        "outputId": "43609c2e-31c1-485a-93f0-fd6c82cce210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----+\n",
            "|      name| key|value|\n",
            "+----------+----+-----+\n",
            "|     James| eye|brown|\n",
            "|     James|hair|black|\n",
            "|   Michael| eye| NULL|\n",
            "|   Michael|hair|brown|\n",
            "|    Robert| eye|black|\n",
            "|    Robert|hair|  red|\n",
            "|Washington| eye| grey|\n",
            "|Washington|hair| grey|\n",
            "| Jefferson| eye|     |\n",
            "| Jefferson|hair|brown|\n",
            "+----------+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KIDO2VoKn15",
        "outputId": "be9a74d8-e81b-4dab-b45b-2fff4ed4341c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import approx_count_distinct,count_distinct\n",
        "print(\"approx_count_distinct: \" + \\\n",
        "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu6t_r08Ktb1",
        "outputId": "97e7776f-bb9b-4b78-bf7a-211026ae7232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "approx_count_distinct: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# countDistinct\n",
        "df2 = df.select(count_distinct(\"department\", \"salary\"))\n",
        "df2.show(truncate=False)\n",
        "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2r1UL_oLS0U",
        "outputId": "623bf49a-326d-423e-cbf6-5a38f3acdbce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+\n",
            "|count(DISTINCT department, salary)|\n",
            "+----------------------------------+\n",
            "|8                                 |\n",
            "+----------------------------------+\n",
            "\n",
            "Distinct Count of Department & Salary: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#source code of PySpark Aggregate examples\n",
        "\n",
        "# Imports\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
        "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
        "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness\n",
        "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
        "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "print(\"approx_count_distinct: \" + \\\n",
        "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
        "\n",
        "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
        "\n",
        "df.select(collect_list(\"salary\")).show(truncate=False)\n",
        "\n",
        "df.select(collect_set(\"salary\")).show(truncate=False)\n",
        "\n",
        "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
        "df2.show(truncate=False)\n",
        "print(\"Distinct Count of Department & Salary: \"+str(df2.collect()[0][0]))\n",
        "\n",
        "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
        "df.select(first(\"salary\")).show(truncate=False)\n",
        "df.select(last(\"salary\")).show(truncate=False)\n",
        "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
        "df.select(max(\"salary\")).show(truncate=False)\n",
        "df.select(min(\"salary\")).show(truncate=False)\n",
        "df.select(mean(\"salary\")).show(truncate=False)\n",
        "df.select(skewness(\"salary\")).show(truncate=False)\n",
        "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
        "    stddev_pop(\"salary\")).show(truncate=False)\n",
        "df.select(sum(\"salary\")).show(truncate=False)\n",
        "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
        "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huFD09CBO_cY",
        "outputId": "539b5106-e403-406a-89ef-31bcf52a4a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "approx_count_distinct: 6\n",
            "avg: 3400.0\n",
            "+------------------------------------------------------------+\n",
            "|collect_list(salary)                                        |\n",
            "+------------------------------------------------------------+\n",
            "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
            "+------------------------------------------------------------+\n",
            "\n",
            "+------------------------------------+\n",
            "|collect_set(salary)                 |\n",
            "+------------------------------------+\n",
            "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
            "+------------------------------------+\n",
            "\n",
            "+----------------------------------+\n",
            "|count(DISTINCT department, salary)|\n",
            "+----------------------------------+\n",
            "|8                                 |\n",
            "+----------------------------------+\n",
            "\n",
            "Distinct Count of Department & Salary: 8\n",
            "count: Row(count(salary)=10)\n",
            "+-------------+\n",
            "|first(salary)|\n",
            "+-------------+\n",
            "|3000         |\n",
            "+-------------+\n",
            "\n",
            "+------------+\n",
            "|last(salary)|\n",
            "+------------+\n",
            "|4100        |\n",
            "+------------+\n",
            "\n",
            "+-------------------+\n",
            "|kurtosis(salary)   |\n",
            "+-------------------+\n",
            "|-0.6467803030303032|\n",
            "+-------------------+\n",
            "\n",
            "+-----------+\n",
            "|max(salary)|\n",
            "+-----------+\n",
            "|4600       |\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|min(salary)|\n",
            "+-----------+\n",
            "|2000       |\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|avg(salary)|\n",
            "+-----------+\n",
            "|3400.0     |\n",
            "+-----------+\n",
            "\n",
            "+--------------------+\n",
            "|skewness(salary)    |\n",
            "+--------------------+\n",
            "|-0.12041791181069571|\n",
            "+--------------------+\n",
            "\n",
            "+-----------------+-------------------+------------------+\n",
            "|stddev(salary)   |stddev_samp(salary)|stddev_pop(salary)|\n",
            "+-----------------+-------------------+------------------+\n",
            "|765.9416862050705|765.9416862050705  |726.636084983398  |\n",
            "+-----------------+-------------------+------------------+\n",
            "\n",
            "+-----------+\n",
            "|sum(salary)|\n",
            "+-----------+\n",
            "|34000      |\n",
            "+-----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/functions.py:988: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
            "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|sum(DISTINCT salary)|\n",
            "+--------------------+\n",
            "|20900               |\n",
            "+--------------------+\n",
            "\n",
            "+-----------------+-----------------+---------------+\n",
            "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
            "+-----------------+-----------------+---------------+\n",
            "|586666.6666666666|586666.6666666666|528000.0       |\n",
            "+-----------------+-----------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        "\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukhHgILgeNIk",
        "outputId": "9c62de74-8e58-4313-be95-844c1dceb011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "windowSpec= Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df.withColumn(\"row_number\",row_number().over(windowS)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOo_dzEfewRi",
        "outputId": "a28880e3-eaef-4797-ecf1-6b4472bdc6d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|        Maria|   Finance|  3000|         1|\n",
            "|        Scott|   Finance|  3300|         2|\n",
            "|          Jen|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|         Jeff| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|        James|     Sales|  3000|         2|\n",
            "|       Robert|     Sales|  4100|         3|\n",
            "|         Saif|     Sales|  4100|         4|\n",
            "|      Michael|     Sales|  4600|         5|\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank\n",
        "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
        "    .show()"
      ],
      "metadata": {
        "outputId": "72a4c219-168b-4fd4-b4c6-f4faab9345a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3ggfazkhb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|   1|\n",
            "|        Scott|   Finance|  3300|   2|\n",
            "|          Jen|   Finance|  3900|   3|\n",
            "|        Kumar| Marketing|  2000|   1|\n",
            "|         Jeff| Marketing|  3000|   2|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|       Robert|     Sales|  4100|   3|\n",
            "|         Saif|     Sales|  4100|   3|\n",
            "|      Michael|     Sales|  4600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dense_rank\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
        "    .show()"
      ],
      "metadata": {
        "outputId": "d0539c27-ded5-489b-ef4c-f6620f508491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdENnYgahYYp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|        Maria|   Finance|  3000|         1|\n",
            "|        Scott|   Finance|  3300|         2|\n",
            "|          Jen|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|         Jeff| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|       Robert|     Sales|  4100|         2|\n",
            "|         Saif|     Sales|  4100|         2|\n",
            "|      Michael|     Sales|  4600|         3|\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import percent_rank\n",
        "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
        "    .show()"
      ],
      "metadata": {
        "outputId": "9ae8c2b3-191b-4f1f-b856-45df87600bcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K6gN-_LhWJ5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+------------+\n",
            "|employee_name|department|salary|percent_rank|\n",
            "+-------------+----------+------+------------+\n",
            "|        Maria|   Finance|  3000|         0.0|\n",
            "|        Scott|   Finance|  3300|         0.5|\n",
            "|          Jen|   Finance|  3900|         1.0|\n",
            "|        Kumar| Marketing|  2000|         0.0|\n",
            "|         Jeff| Marketing|  3000|         1.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|       Robert|     Sales|  4100|         0.5|\n",
            "|         Saif|     Sales|  4100|         0.5|\n",
            "|      Michael|     Sales|  4600|         1.0|\n",
            "+-------------+----------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        "\n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import rank\n",
        "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
        "    .show()\n",
        "\n",
        "from pyspark.sql.functions import dense_rank\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
        "    .show()\n",
        "\n",
        "from pyspark.sql.functions import percent_rank\n",
        "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
        "    .show()\n",
        "\n",
        "from pyspark.sql.functions import ntile\n",
        "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
        "    .show()\n",
        "\n",
        "from pyspark.sql.functions import cume_dist\n",
        "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
        "   .show()\n",
        "\n",
        "from pyspark.sql.functions import lag\n",
        "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
        "      .show()\n",
        "\n",
        "from pyspark.sql.functions import lead\n",
        "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
        "    .show()\n",
        "\n",
        "windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
        "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
        "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
        "  .show()"
      ],
      "metadata": {
        "outputId": "97899a8c-486e-4222-9b8d-5abb54b38f03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOXXwxnrhU18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Maria        |Finance   |3000  |1         |\n",
            "|Scott        |Finance   |3300  |2         |\n",
            "|Jen          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Jeff         |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|James        |Sales     |3000  |2         |\n",
            "|Robert       |Sales     |4100  |3         |\n",
            "|Saif         |Sales     |4100  |4         |\n",
            "|Michael      |Sales     |4600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|   1|\n",
            "|        Scott|   Finance|  3300|   2|\n",
            "|          Jen|   Finance|  3900|   3|\n",
            "|        Kumar| Marketing|  2000|   1|\n",
            "|         Jeff| Marketing|  3000|   2|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|       Robert|     Sales|  4100|   3|\n",
            "|         Saif|     Sales|  4100|   3|\n",
            "|      Michael|     Sales|  4600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n",
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|        Maria|   Finance|  3000|         1|\n",
            "|        Scott|   Finance|  3300|         2|\n",
            "|          Jen|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|         Jeff| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|       Robert|     Sales|  4100|         2|\n",
            "|         Saif|     Sales|  4100|         2|\n",
            "|      Michael|     Sales|  4600|         3|\n",
            "+-------------+----------+------+----------+\n",
            "\n",
            "+-------------+----------+------+------------+\n",
            "|employee_name|department|salary|percent_rank|\n",
            "+-------------+----------+------+------------+\n",
            "|        Maria|   Finance|  3000|         0.0|\n",
            "|        Scott|   Finance|  3300|         0.5|\n",
            "|          Jen|   Finance|  3900|         1.0|\n",
            "|        Kumar| Marketing|  2000|         0.0|\n",
            "|         Jeff| Marketing|  3000|         1.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|       Robert|     Sales|  4100|         0.5|\n",
            "|         Saif|     Sales|  4100|         0.5|\n",
            "|      Michael|     Sales|  4600|         1.0|\n",
            "+-------------+----------+------+------------+\n",
            "\n",
            "+-------------+----------+------+-----+\n",
            "|employee_name|department|salary|ntile|\n",
            "+-------------+----------+------+-----+\n",
            "|        Maria|   Finance|  3000|    1|\n",
            "|        Scott|   Finance|  3300|    1|\n",
            "|          Jen|   Finance|  3900|    2|\n",
            "|        Kumar| Marketing|  2000|    1|\n",
            "|         Jeff| Marketing|  3000|    2|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|       Robert|     Sales|  4100|    1|\n",
            "|         Saif|     Sales|  4100|    2|\n",
            "|      Michael|     Sales|  4600|    2|\n",
            "+-------------+----------+------+-----+\n",
            "\n",
            "+-------------+----------+------+------------------+\n",
            "|employee_name|department|salary|         cume_dist|\n",
            "+-------------+----------+------+------------------+\n",
            "|        Maria|   Finance|  3000|0.3333333333333333|\n",
            "|        Scott|   Finance|  3300|0.6666666666666666|\n",
            "|          Jen|   Finance|  3900|               1.0|\n",
            "|        Kumar| Marketing|  2000|               0.5|\n",
            "|         Jeff| Marketing|  3000|               1.0|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|       Robert|     Sales|  4100|               0.8|\n",
            "|         Saif|     Sales|  4100|               0.8|\n",
            "|      Michael|     Sales|  4600|               1.0|\n",
            "+-------------+----------+------+------------------+\n",
            "\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary| lag|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|NULL|\n",
            "|        Scott|   Finance|  3300|NULL|\n",
            "|          Jen|   Finance|  3900|3000|\n",
            "|        Kumar| Marketing|  2000|NULL|\n",
            "|         Jeff| Marketing|  3000|NULL|\n",
            "|        James|     Sales|  3000|NULL|\n",
            "|        James|     Sales|  3000|NULL|\n",
            "|       Robert|     Sales|  4100|3000|\n",
            "|         Saif|     Sales|  4100|3000|\n",
            "|      Michael|     Sales|  4600|4100|\n",
            "+-------------+----------+------+----+\n",
            "\n",
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|lead|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|3900|\n",
            "|        Scott|   Finance|  3300|NULL|\n",
            "|          Jen|   Finance|  3900|NULL|\n",
            "|        Kumar| Marketing|  2000|NULL|\n",
            "|         Jeff| Marketing|  3000|NULL|\n",
            "|        James|     Sales|  3000|4100|\n",
            "|        James|     Sales|  3000|4100|\n",
            "|       Robert|     Sales|  4100|4600|\n",
            "|         Saif|     Sales|  4100|NULL|\n",
            "|      Michael|     Sales|  4600|NULL|\n",
            "+-------------+----------+------+----+\n",
            "\n",
            "+----------+------+-----+----+----+\n",
            "|department|   avg|  sum| min| max|\n",
            "+----------+------+-----+----+----+\n",
            "|   Finance|3400.0|10200|3000|3900|\n",
            "| Marketing|2500.0| 5000|2000|3000|\n",
            "|     Sales|3760.0|18800|3000|4600|\n",
            "+----------+------+-----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "            .appName('SparkByExamples.com') \\\n",
        "            .getOrCreate()\n",
        "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
        "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "df.show()"
      ],
      "metadata": {
        "outputId": "97379dfa-dd23-4fe2-8be7-63fae0ffa1db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKwPLV1khSqr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| id|     input|\n",
            "+---+----------+\n",
            "|  1|2020-02-01|\n",
            "|  2|2019-03-01|\n",
            "|  3|2021-03-01|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#current_date()\n",
        "df.select(current_date().alias(\"current_date\")\n",
        "  ).show(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOWoJhVXhPdV",
        "outputId": "af26a0d5-a8a6-42ee-941c-0996748018c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+\n",
            "|current_date|\n",
            "+------------+\n",
            "|  2024-08-28|\n",
            "+------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#date_format()\n",
        "df.select(col(\"input\"),\n",
        "    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\")\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYrW2MbglwUs",
        "outputId": "83b411e9-310c-488e-ee38-e6a9321622a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+\n",
            "|     input|date_format|\n",
            "+----------+-----------+\n",
            "|2020-02-01| 02-01-2020|\n",
            "|2019-03-01| 03-01-2019|\n",
            "|2021-03-01| 03-01-2021|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to_date()\n",
        "df.select(col(\"input\"),\n",
        "    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\")\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upQCvhT6l0D5",
        "outputId": "1a7f6098-ea23-4ef3-ce5f-1a50bbffc5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|     input|   to_date|\n",
            "+----------+----------+\n",
            "|2020-02-01|2020-02-01|\n",
            "|2019-03-01|2019-03-01|\n",
            "|2021-03-01|2021-03-01|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#datediff()\n",
        "df.select(col(\"input\"),\n",
        "    datediff(current_date(),col(\"input\")).alias(\"datediff\")\n",
        "  ).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl_D9eM6mCPy",
        "outputId": "df9a4242-a7e0-4c86-88af-1c5f5cc44b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+\n",
            "|     input|datediff|\n",
            "+----------+--------+\n",
            "|2020-02-01|    1670|\n",
            "|2019-03-01|    2007|\n",
            "|2021-03-01|    1276|\n",
            "+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#months_between()\n",
        "df.select(col(\"input\"),\n",
        "    months_between(current_date(),col(\"input\")).alias(\"months_between\")\n",
        "  ).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11yLUWFomx-x",
        "outputId": "2463fcaf-f9c7-4e52-d5d9-16b035eafee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+\n",
            "|     input|months_between|\n",
            "+----------+--------------+\n",
            "|2020-02-01|   54.87096774|\n",
            "|2019-03-01|   65.87096774|\n",
            "|2021-03-01|   41.87096774|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(col(\"input\"),\n",
        "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"),\n",
        "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"),\n",
        "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
        "   ).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LthFMSnhm7Fr",
        "outputId": "cd944d89-c3cc-4c9a-bbc8-b9f871d34923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+----------+-----------+\n",
            "|     input|Month_Trunc|Month_Year|Month_Trunc|\n",
            "+----------+-----------+----------+-----------+\n",
            "|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n",
            "|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n",
            "|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n",
            "+----------+-----------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add_months() , date_add(), date_sub()\n",
        "df.select(col(\"input\"),\n",
        "    add_months(col(\"input\"),3).alias(\"add_months\"),\n",
        "    add_months(col(\"input\"),-3).alias(\"sub_months\"),\n",
        "    date_add(col(\"input\"),4).alias(\"date_add\"),\n",
        "    date_sub(col(\"input\"),4).alias(\"date_sub\")\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKoDiuirnnUi",
        "outputId": "c9065f8b-3b41-4fa6-efd1-d5730bc4883e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+----------+----------+----------+\n",
            "|     input|add_months|sub_months|  date_add|  date_sub|\n",
            "+----------+----------+----------+----------+----------+\n",
            "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
            "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
            "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
            "+----------+----------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.select(col(\"input\"),\n",
        "     year(col(\"input\")).alias(\"year\"),\n",
        "     month(col(\"input\")).alias(\"month\"),\n",
        "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"),\n",
        "     weekofyear(col(\"input\")).alias(\"weekofyear\")\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQbxGUIknqMH",
        "outputId": "27ee5c54-b14c-42f0-9c70-8360ed4d792f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+-----+----------+----------+\n",
            "|     input|year|month|  next_day|weekofyear|\n",
            "+----------+----+-----+----------+----------+\n",
            "|2020-02-01|2020|    2|2020-02-02|         5|\n",
            "|2019-03-01|2019|    3|2019-03-03|         9|\n",
            "|2021-03-01|2021|    3|2021-03-07|         9|\n",
            "+----------+----+-----+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(col(\"input\"),\n",
        "     dayofweek(col(\"input\")).alias(\"dayofweek\"),\n",
        "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"),\n",
        "     dayofyear(col(\"input\")).alias(\"dayofyear\"),\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mTNecZEnxdi",
        "outputId": "62e5ddd7-73eb-4f27-d6f5-596b60c1f670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+---------+\n",
            "|     input|dayofweek|dayofmonth|dayofyear|\n",
            "+----------+---------+----------+---------+\n",
            "|2020-02-01|        7|         1|       32|\n",
            "|2019-03-01|        6|         1|       60|\n",
            "|2021-03-01|        2|         1|       60|\n",
            "+----------+---------+----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
        "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LYvGVVBoBFi",
        "outputId": "80f26a54-9823-4a92-f459-a216f33ed952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------------+\n",
            "|id |input                  |\n",
            "+---+-----------------------+\n",
            "|1  |02-01-2020 11 01 19 06 |\n",
            "|2  |03-01-2019 12 01 19 406|\n",
            "|3  |03-01-2021 12 01 19 406|\n",
            "+---+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#current_timestamp()\n",
        "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
        "  ).show(1,truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJoFCZL1oDW5",
        "outputId": "1b235e9a-96ad-48cf-be0b-b3c608616ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+\n",
            "|current_timestamp         |\n",
            "+--------------------------+\n",
            "|2024-08-28 15:34:56.367493|\n",
            "+--------------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to_timestamp()\n",
        "df2.select(col(\"input\"),\n",
        "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")\n",
        "  ).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "xHy8M1O1oMrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hour, minute,second\n",
        "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
        "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "\n",
        "df3.select(col(\"input\"),\n",
        "    hour(col(\"input\")).alias(\"hour\"),\n",
        "    minute(col(\"input\")).alias(\"minute\"),\n",
        "    second(col(\"input\")).alias(\"second\")\n",
        "  ).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "LiVMgbswoPwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession,Row\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "jsonString=\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"\n",
        "df=spark.createDataFrame([(1, jsonString)],[\"id\",\"value\"])\n",
        "df.show(truncate=False)\n",
        "\n",
        "#Convert JSON string column to Map type\n",
        "from pyspark.sql.types import MapType,StringType\n",
        "from pyspark.sql.functions import from_json\n",
        "df2=df.withColumn(\"value\",from_json(df.value,MapType(StringType(),StringType())))\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import to_json,col\n",
        "df2.withColumn(\"value\",to_json(col(\"value\"))) \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import json_tuple\n",
        "df.select(col(\"id\"),json_tuple(col(\"value\"),\"Zipcode\",\"ZipCodeType\",\"City\")) \\\n",
        "    .toDF(\"id\",\"Zipcode\",\"ZipCodeType\",\"City\") \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import get_json_object\n",
        "df.select(col(\"id\"),get_json_object(col(\"value\"),\"$.ZipCodeType\").alias(\"ZipCodeType\")) \\\n",
        "    .show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import schema_of_json,lit\n",
        "schemaStr=spark.range(1) \\\n",
        "    .select(schema_of_json(lit(\"\"\"{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}\"\"\"))) \\\n",
        "    .collect()[0][0]\n",
        "print(schemaStr)"
      ],
      "metadata": {
        "id": "rE7NrrVbQRIW",
        "outputId": "ddd5154d-4f9b-46f3-ad99-c77450d5fe12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------------------------------------------------+\n",
            "|id |value                                                                     |\n",
            "+---+--------------------------------------------------------------------------+\n",
            "|1  |{\"Zipcode\":704,\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
            "+---+--------------------------------------------------------------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- value: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+---+---------------------------------------------------------------------------+\n",
            "|id |value                                                                      |\n",
            "+---+---------------------------------------------------------------------------+\n",
            "|1  |{Zipcode -> 704, ZipCodeType -> STANDARD, City -> PARC PARQUE, State -> PR}|\n",
            "+---+---------------------------------------------------------------------------+\n",
            "\n",
            "+---+----------------------------------------------------------------------------+\n",
            "|id |value                                                                       |\n",
            "+---+----------------------------------------------------------------------------+\n",
            "|1  |{\"Zipcode\":\"704\",\"ZipCodeType\":\"STANDARD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}|\n",
            "+---+----------------------------------------------------------------------------+\n",
            "\n",
            "+---+-------+-----------+-----------+\n",
            "|id |Zipcode|ZipCodeType|City       |\n",
            "+---+-------+-----------+-----------+\n",
            "|1  |704    |STANDARD   |PARC PARQUE|\n",
            "+---+-------+-----------+-----------+\n",
            "\n",
            "+---+-----------+\n",
            "|id |ZipCodeType|\n",
            "+---+-----------+\n",
            "|1  |STANDARD   |\n",
            "+---+-----------+\n",
            "\n",
            "STRUCT<City: STRING, State: STRING, ZipCodeType: STRING, Zipcode: BIGINT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "#spark = SparkSession.builder().master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
        "\n",
        "# Read CSV File\n",
        "df = spark.read.csv(\"zipcodes.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "O10uRn_6SQ_4",
        "outputId": "54b981a6-b253-4f72-9d16-38bf1ea712ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            " |-- _c7: string (nullable = true)\n",
            " |-- _c8: string (nullable = true)\n",
            " |-- _c9: string (nullable = true)\n",
            " |-- _c10: string (nullable = true)\n",
            " |-- _c11: string (nullable = true)\n",
            " |-- _c12: string (nullable = true)\n",
            " |-- _c13: string (nullable = true)\n",
            " |-- _c14: string (nullable = true)\n",
            " |-- _c15: string (nullable = true)\n",
            " |-- _c16: string (nullable = true)\n",
            " |-- _c17: string (nullable = true)\n",
            " |-- _c18: string (nullable = true)\n",
            " |-- _c19: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = os.getcwd()\n",
        "print(path)"
      ],
      "metadata": {
        "id": "pHRYG699TZA9",
        "outputId": "0c553761-5cea-48fe-9eee-08cc16c81b4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
        "\n",
        "# Using custom schema\n",
        "schema = StructType() \\\n",
        "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
        "      .add(\"Zipcode\",IntegerType(),True) \\\n",
        "      .add(\"ZipCodeType\",StringType(),True) \\\n",
        "      .add(\"City\",StringType(),True) \\\n",
        "      .add(\"State\",StringType(),True) \\\n",
        "      .add(\"LocationType\",StringType(),True) \\\n",
        "      .add(\"Lat\",DoubleType(),True) \\\n",
        "      .add(\"Long\",DoubleType(),True) \\\n",
        "      .add(\"Xaxis\",IntegerType(),True) \\\n",
        "      .add(\"Yaxis\",DoubleType(),True) \\\n",
        "      .add(\"Zaxis\",DoubleType(),True) \\\n",
        "      .add(\"WorldRegion\",StringType(),True) \\\n",
        "      .add(\"Country\",StringType(),True) \\\n",
        "      .add(\"LocationText\",StringType(),True) \\\n",
        "      .add(\"Location\",StringType(),True) \\\n",
        "      .add(\"Decommisioned\",BooleanType(),True) \\\n",
        "      .add(\"TaxReturnsFiled\",StringType(),True) \\\n",
        "      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n",
        "      .add(\"TotalWages\",IntegerType(),True) \\\n",
        "      .add(\"Notes\",StringType(),True)\n",
        "\n",
        "df_with_schema = spark.read.format(\"csv\") \\\n",
        "      .option(\"header\", True) \\\n",
        "      .schema(schema) \\\n",
        "      .load(\"zipcodes.csv\")"
      ],
      "metadata": {
        "id": "tvpH5LOPUunX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName(\"parquetFile\").getOrCreate()\n",
        "data =[(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
        "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
        "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
        "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
        "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
        "columns=[\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
        "df=spark.createDataFrame(data,columns)"
      ],
      "metadata": {
        "id": "lUKftS1kWPzZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet(\"people.parquet\")"
      ],
      "metadata": {
        "id": "itfMV6tSWTHv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parDF=spark.read.parquet(\"people.parquet\")"
      ],
      "metadata": {
        "id": "h_9gHZkAWgOR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Using spark.sql\n",
        "parDF.createOrReplaceTempView(\"ParquetTable\")\n",
        "parkSQL = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
        "parkSQL.show()"
      ],
      "metadata": {
        "id": "8GSiSQXyW1-j",
        "outputId": "f0b9f337-8cdd-49ab-95c9-f0f3e7c0f8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+-----+------+------+\n",
            "|firstname|middlename|lastname|  dob|gender|salary|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "|  Robert |          |Williams|42114|     M|  4000|\n",
            "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
            "| Michael |      Rose|        |40288|     M|  4000|\n",
            "+---------+----------+--------+-----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"people2.parquet\")"
      ],
      "metadata": {
        "id": "fZAp_e78XgVi"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWFEfbSQXuY6qKxXXvTIBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}